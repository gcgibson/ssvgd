\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Sequential Stein Variational Gradient Descent for Time Series Model Estimation},
            pdfauthor={Gibson, Reich, and Ray in some order},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}
  \title{Sequential Stein Variational Gradient Descent for Time Series Model
Estimation}
  \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \author{Gibson, Reich, and Ray in some order}
  \preauthor{\centering\large\emph}
  \postauthor{\par}
  \predate{\centering\large\emph}
  \postdate{\par}
  \date{December 3, 2017}

\usepackage{multicol}
\usepackage{amssymb}

\begin{document}
\maketitle

\section{Introduction}\label{introduction}

Particle filtering suffers from two main practical disadvantages. The
first is particle depletion, where the number of effective particles
with non-neglibile weight becomes too small. This has the effect of
concentrating the mass around a small number of particles, leading to
poor estimates of the target distribution. The second is the running
time of the algorithm. A cursory analysis reveals that each particle is
updated once per time step in the time series, and once per re-sampling
step, to mitigate the issue above. If we imagine the order of particles
is close to the length of the time series, we see that run-time is
\(O(n^3)\).

We propose another approach that we hope will do better than particle
filtering in practice. In this approach, Stein Variational Gradient
Descent (SVGD) is used to sequentially estimate the distribution of
state variables in each time step, conditional on observed data up
through that time. This method should overcome problems with particle
depletion and excessive run-times for long time-series.

\subsection{Overview of SVGD}\label{overview-of-svgd}

Stein Variational Gradient Descent can be used to estimate a continuous
distribution by a set of samples. By iteratively transporting samples
from an initial distribution in the direction of the likelihood, we are
able to generate a sample from the target distribution. The usefullness
of this approximation is apparent in Bayesian statistics, where the
usually intractable normalizing constant disappears in the gradient. The
particles are subject to the following gradient ascent procedure.

\[x_t^{(i)} \leftarrow x_{t-1}^{(i)} + \frac{1}{n}\sum_{j=1}^n[ k(x_j,x_{t-1}^{(i)})*\nabla log\ p(x_j) + \nabla k(x_j,,x_{t-1}^{(i)})]\]

\subsection{Sequential Stein Variational Gradient
Descent}\label{sequential-stein-variational-gradient-descent}

Suppose we are given a time series \(Y_1,Y_2,...,Y_t\) for
\(Y \in \mathbb{R}\). We model the sequence as a state-space model
parameterized by an observation density \(p(y_t | x_t)\) and a
transition density \(p(x_t | x_{t-1})\) Figure 1.

\begin{figure}[htbp]
\centering
\includegraphics{/Users/gcgibson/Desktop/ssm.png}
\caption{Caption for the picture.}
\end{figure}

We are interested in the filtering distribution
\(p(x_1,...,x_n | y_1,...,y_n)\) which by Bayes formula is
\[p(x_1,...,x_n | y_1,...,y_n) = \frac{p(y_1,...,y_n | x_1,...,x_n) p(x_1,...,x_n)}{Z}\].

Because computing the normalizing constant \(Z\) is intractable for many
choices of \(p(y_t | x_t)\) and \(p(x_t | x_{t-1})\), we must resort to
monte carlo algorithms. The classic approach that incorporates the
sequential nature of the data is given by the particle filtering
algorithm. Particle filtering approximates the filtering density using
sequential importance sampling. We instead focus on the following
recursion.

\[p(x_t | y_{1:t}) = \int p(x_{0:t} | y_{1:t})d_{x_0:t-1}\]
\[=\frac{p(y_t | x_t)}{\int p(y_t|x_t)p(x_t | y_{1:t-1})dx_t}p(x_t | y_{1:t-1})\]

\[\propto p(y_t|x_t)p(x_t | y_{1:t-1})\]
\[\propto p(y_t|x_t)p(x_t | y_{1:t-1})\]
\[\propto p(y_t|x_t)\int_{x_{t-1}}p(x_t,x_{t-1} | y_{1:t-1})d_{x_{t-1}}\]

\[\propto p(y_t|x_t)\int_{x_{t-1}}p(x_t |x_{t-1} )p(x_{t-1}| y_{1:t-1})d_{x_{t-1}}\]

which we can approximate recursively as
\[\propto p(y_t|x_t) \frac{1}{n}\sum_{i=1}^n p(x_t | x_{t-1}^{(i)})\]

(proof in apendix A)

\subsection{Model Structure}\label{model-structure}

States:

\begin{itemize}
\item $X_1 \sim g_1(x_1 ; \xi)$
\item $X_t \vert X_{t-1} \sim g(x_t \vert x_{t - 1} ; \xi)$ for all $t = 2, \ldots, T$
\end{itemize}

Observations:

\begin{itemize}
\item $Y_t \vert X_{t} \sim h(y_t | x_t ; \zeta)$
\end{itemize}

Here, \(g_1(\cdot)\) and \(g(\cdot)\) are appropriately defined
probability density functions depending on parameters \(\xi\) and
\(h(\cdot)\) is an appropriately defined probability density function or
probability mass function depending on parameters \(\zeta\).

Define \(\theta = (\xi, \zeta)\) to be the full set of model parameters.

\subsection{Filtering}\label{filtering}

There are two types of filtering:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  sample of particles \(x_{1:T}^{(k)} \sim f(x_{1:T} | y_{1:T})\)
\item
  sample of particles \(x_{t}^{(k)} \sim f(x_{t} | y_{1:t})\) for each
  \(t = 1, \ldots, T\)
\end{enumerate}

Let's look at the second one. Assume we have a sample
\(x_{t-1}^{(k)} \sim f(x_{t-1} | y_{1:t-1})\)

\begin{align*}
p(x_{t} | y_{1:t}) &= \frac{f(x_t, y_t | y_{1:t-1})}{f(y_t | y_{1:t-1})} \\
 &\propto f(x_t, y_t | y_{1:t-1}) \\
 &= f(y_t | x_t) f(x_t | y_{1:t-1}) \\
 &= f(y_t | x_t) \int f(x_t, x_{t-1} | y_{1:t-1}) d x_{t-1} \\
 &= f(y_t | x_t) \int f(x_t | x_{t - 1}) f(x_{t-1} | y_{1:{t-1}}) dx_{t-1} \\
 &\approx f(y_t | x_t) \sum_{x_{t-1}^{(k)}} f(x_t | x_{t - 1}^{(k)})
\end{align*}

So \(\log\{p(x_{t} | y_{1:t})\}\) is approximately proportional to
\(\log\{f(y_t | x_t)\} + \log\{\sum_{x_{t-1}^{(k)}} f(x_t | x_{t - 1}^{(k)})\}\)

\subsection{Evaluating the Likelihood via
Filtering}\label{evaluating-the-likelihood-via-filtering}

Our goal (for now) is to evaluate the likelihood function

\begin{align*}
L(\theta \vert y_{1:T}) &= f(y_{1:T} ; \theta) \\
&= f(y_1 ; \theta) \prod_{t = 2}^T f(y_t \vert y_{1:t-1} ; \theta) \\
&= \int_{x_1} f(y_1, x_1 ; \theta) d x_1 \prod_{t = 2}^T \int_{x_t} f(y_t, x_t \vert y_{1:t-1} ; \zeta) d x_{t} \\
&= \int_{x_1} f(y_1 \vert x_1 ; \zeta) f(x_1 ; \xi) d x_1 \prod_{t = 2}^T \int_{x_t} f(y_t \vert x_t, y_{1:t-1} ; \zeta) f(x_t \vert y_{1:t-1} ;\xi) d x_t \\
&= \int_{x_1} f(y_1 \vert x_1 ; \zeta) f(x_1 ; \xi) d x_1 \prod_{t = 2}^T \int_{x_t} f(y_t \vert x_t ; \zeta) f(x_t \vert y_{1:t-1} ;\xi) d x_t \\
&\approx \sum_{x_1^{(k)}} f(y_1 \vert x_1^{(k)} ; \zeta) \prod_{t = 2}^T \sum_{x_{t|t-1}^{(k)}} f(y_t \vert x_{t|t-1}^{(k)} ; \zeta) \text{, where}
\end{align*}

\begin{align*}
x_1^{(k)} \sim f(x_1 ; \xi) \text{ and }
x_{t|t-1}^{(k)} \sim f(x_t \vert y_{1:t-1} ;\xi)
\end{align*}

Note that if we have a sample
\(x_{t-1|t-1}^{(k)} \sim f(x_{t-1} \vert y_{1:t-1} ;\xi)\), we can
obtain a sample \(x_{t|t-1}^{(k)} \sim f(x_t \vert y_{1:t-1} ;\xi)\)
from the transition density.

We will apply SVGD to iteratively obtain samples from the updated
distributions \(x_{t|t}^{(k)} \sim f(x_{t} \vert y_{1:t} ;\xi)\)
starting from samples
\(x_{t-1|t-1}^{(k)} \sim f(x_{t-1} \vert y_{1:t-1} ;\xi)\) at the
previous time step. To do this, we need to obtain the derivative of the
log of the density we want to estimate with respect to \(x_{t}\).

\begin{align*}
&\frac{d}{d x_t} \log\{f(x_{t} \vert y_{1:t}; \xi)\} = \frac{d}{d x_t} \log\left\{\frac{f(x_t \vert y_{1:t-1}) f(y_t \vert x_t, y_{1:t-1})}{f(y_t \vert y_{t:t-1})}\right\} \\
&\qquad = \frac{d}{d x_t} \left[ \log\left\{f(x_t \vert y_{1:t-1})\right\} + \log \left\{f(y_t \vert x_t)\right\} - \log\left\{f(y_t \vert y_{t:t-1})\right\} \right] \\
&\qquad = \frac{d}{d x_t} \log\left\{\int_{x_{t-1}}f(x_{t} \vert x_{t-1}, y_{1:t-1}; \xi)f(x_{t-1} \vert y_{1:t-1}; \xi) d x_{t-1} \right\} + \frac{d}{d x_t} \log \left\{ f(y_t \vert x_t) \right\} \\
&\qquad = \frac{\frac{d}{d x_t} \int_{x_{t-1}}f(x_{t} \vert x_{t-1}; \xi)f(x_{t-1} \vert y_{1:t-1}; \xi) d x_{t-1}}{\int_{x_{t-1}}f(x_{t} \vert x_{t-1}; \xi)f(x_{t-1} \vert y_{1:t-1}; \xi) d x_{t-1}} + \frac{\frac{d}{d x_t} f(y_t \vert x_t)}{f(y_t \vert x_t)} \\
&\qquad \approx \frac{\frac{d}{d x_t} \sum_{x_{t-1|t-1}^{(k)}}f(x_{t} \vert x_{t-1}; \xi)}{\sum_{x_{t-1|t-1}^{(k)}}f(x_{t} \vert x_{t-1}; \xi)} + \frac{\frac{d}{d x_t} f(y_t \vert x_t)}{f(y_t \vert x_t)} \\
&\qquad = \frac{\sum_{x_{t-1|t-1}^{(k)}} \frac{d}{d x_t} f(x_{t} \vert x_{t-1}; \xi)}{\sum_{x_{t-1|t-1}^{(k)}}f(x_{t} \vert x_{t-1}; \xi)} + \frac{\frac{d}{d x_t} f(y_t \vert x_t)}{f(y_t \vert x_t)}
\end{align*}

\subsection{Simulation Studies}\label{simulation-studies}

We will do several simulation studies, divided into 2 groups:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  illustrating scenarios in which common particle filtering methods
  struggle, but SSVGD has better chances

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    bad initalization
  \item
    normal-poisson filtering, seasonal model
  \item
    one other, more complex?
  \end{enumerate}
\item
  demonstrating accuracy (compare to true states, exactly computed
  filtered states, and likelihood)

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Kalman filter
  \item
    something nonlinear where we can do exact computations (brute force
    for short time series??)?
  \end{enumerate}
\end{enumerate}

\subsubsection{1. a. Bad Initialization}\label{a.-bad-initialization}

\paragraph{Simulation Study Design}\label{simulation-study-design}

Paragraph with data generating process, written with math.

Paragraph describing settings for simulation, e.g.~number of simulation
runs, length of time series generated, etc.

Paragraph describing different methods in comparison. 2 PF
implementations, one non-linear KF implementation, and SSVGD

\paragraph{Results}\label{results}

Paragraph, referencing one figure and one table, summarizing results

\subsubsection{1. b. Normal-Poisson seasonal
model}\label{b.-normal-poisson-seasonal-model}

\paragraph{Simulation Study Design}\label{simulation-study-design-1}

Paragraph with data generating process, written with math.

Paragraph describing settings for simulation, e.g.~number of simulation
runs, length of time series generated, etc.

Paragraph describing different methods in comparison. 2 PF
implementations, one non-linear KF implementation, and SSVGD

\paragraph{Results}\label{results-1}

Paragraph, referencing one figure and one table, summarizing results

\subsubsection{1. c. One other, more
complex?}\label{c.-one-other-more-complex}

\paragraph{Simulation Study Design}\label{simulation-study-design-2}

Paragraph with data generating process, written with math.

Paragraph describing settings for simulation, e.g.~number of simulation
runs, length of time series generated, etc.

Paragraph describing different methods in comparison. 2 PF
implementations, one non-linear KF implementation, and SSVGD

\paragraph{Results}\label{results-2}

Paragraph, referencing one figure and one table, summarizing results

\subsubsection{2. a. Kalman Filter}\label{a.-kalman-filter}

\paragraph{Simulation Study Design}\label{simulation-study-design-3}

Paragraph with data generating process, written with math.

Paragraph describing settings for simulation, e.g.~number of simulation
runs, length of time series generated, etc.

Paragraph describing different methods in comparison. 2 PF
implementations, one non-linear KF implementation, and SSVGD

\paragraph{Results}\label{results-3}

Paragraph, referencing one figure and one table, summarizing results

\subsubsection{2. b. something nonlinear where we can do exact
computations (brute force for short time
series??)?}\label{b.-something-nonlinear-where-we-can-do-exact-computations-brute-force-for-short-time-series}

\paragraph{Simulation Study Design}\label{simulation-study-design-4}

Paragraph with data generating process, written with math.

Paragraph describing settings for simulation, e.g.~number of simulation
runs, length of time series generated, etc.

Paragraph describing different methods in comparison. 2 PF
implementations, one non-linear KF implementation, and SSVGD

\paragraph{Results}\label{results-4}

Paragraph, referencing one figure and one table, summarizing results

\subsection{Application}\label{application}

Example model with real data. fairly real model, but not thaaaaaat
complex.

\subsection{Discussion}\label{discussion}

\subsection{Bibliography}\label{bibliography}


\end{document}
