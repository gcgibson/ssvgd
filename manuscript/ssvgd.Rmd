---
title: 'Sequential Stein Variational Gradient Descent for Time Series Model Estimation'
author: "Gibson, Reich, and Ray in some order"
date: "December 3, 2017"
output:
  pdf_document:
    fig_height: 2.7
    fig_width: 6.5
    keep_tex: yes
header-includes:
   - \usepackage{multicol}
   - \usepackage{amssymb}
   - \usepackage{amsmath}
---

```{r setup, include=FALSE}
#knitr::opts_chunk$set(cache=TRUE)
```


```{r, include=FALSE}
require(knitr)
library(tidyverse)
library(RefManageR)

options(digits=4)

opts_chunk$set(
  tidy=FALSE,     # display code as typed
  size="small"    # slightly smaller font for code
)
```

# Introduction

State-Space models have become popular tools in the analysis of time-series. They allow for arbitrary transition and observation dynamics. The researcher can assign a latent data generating process, while simultaneously allowing for observational error on that process. The classic algorithm for fitting non-Gaussian SSMs is given by the particle filter. Although many variations exist, we generally refer to the sampling importance re-sampling (SIR) filter when discussing particle filtering. Although a powerful inference tool, particle filtering suffers from several well known drawbacks. The first is the problem of filter degeneracy. This occurs when the observations are far from the state predicted by the latent dynamics. The second is the excessive run-times on longer time series with complex dynamics. 
We propose an alternative approach that we hope will do better than particle filtering in practice.  In this approach, Stein Variational Gradient Descent (SVGD) is used to sequentially estimate the distribution of state variables in each time step, conditional on observed data up through that time.  

## Overview of SVGD
Stein Variational Gradient Descent can be used to estimate a continuous distribution by a set of particles. By iteratively transporting samples from an initial distribution in the direction of the likelihood, we are able to generate compute Monte Carlo estimates of the posterior. The usefulness of this approximation is apparent in Bayesian statistics, where the usually intractable normalizing constant disappears in the particle update step. The particles are subject to the following gradient ascent procedure. 

$$x_i^{l+i} \leftarrow x_i^{l}+\epsilon_l\hat{\phi^*(x_i^l)}   $$
$$\hat{\phi^*(x)} = \frac{1}{n}\sum_{j=1}^n[k(x_j^l,x)\nabla_{x_j^l}log\ p(x_j^l) + \nabla_{x_j^l}k(x_j^l,x)]$$



for an arbitrary positive definite kernel function $k(.,.)$ usually chosen to be a Gaussian kernel.


## State Space Models
Suppose we are given a time series $Y_1,Y_2,...,Y_t$ for $Y \in \mathbb{R}$. We model the sequence as a state-space model parameterized by an observation density $p(y_t | x_t)$ and a transition density $p(x_t | x_{t-1})$ Figure 1.

![State-Space Model Setup](/home/gcgibson/ssm.png)




We are interested in the filtering distribution $p(x_1,...,x_n | y_1,...,y_n)$ which by Bayes formula is $$p(x_1,...,x_n | y_1,...,y_n) = \frac{p(y_1,...,y_n | x_1,...,x_n) p(x_1,...,x_n)}{Z}$$.

Because computing the normalizing constant $Z$ is intractable for many choices of $p(y_t | x_t)$ and $p(x_t | x_{t-1})$, we must resort to Monte Carlo algorithms. The classic approach that incorporates the sequential nature of the data is given by the particle filtering algorithm. Particle filtering approximates the filtering density using sequential importance sampling. We instead focus on the following recursion. 

$$p(x_t | y_{1:t}) = \int p(x_{0:t} | y_{1:t})d_{x_0:t-1}$$
$$=\frac{p(y_t | x_t)}{\int p(y_t|x_t)p(x_t | y_{1:t-1})dx_t}p(x_t | y_{1:t-1})$$

$$\propto p(y_t|x_t)p(x_t | y_{1:t-1})$$
$$\propto p(y_t|x_t)p(x_t | y_{1:t-1})$$
$$\propto p(y_t|x_t)\int_{x_{t-1}}p(x_t,x_{t-1} | y_{1:t-1})d_{x_{t-1}}$$

$$\propto p(y_t|x_t)\int_{x_{t-1}}p(x_t |x_{t-1} )p(x_{t-1}| y_{1:t-1})d_{x_{t-1}}$$

which we can approximate using svgd as 
$$\approx p(y_t|x_t) \frac{1}{n}\sum_{i=1}^n p(x_t | x_{t-1}^{(i)})$$
We can now estimate $p(x_{t+1}|y_{1:t+1})$ using the same algebra as above. 
(proof in apendix A) 






## Model Structure

States:
\begin{itemize}
\item $X_1 \sim g_1(x_1 ; \xi)$
\item $X_t \vert X_{t-1} \sim g(x_t \vert x_{t - 1} ; \xi)$ for all $t = 2, \ldots, T$
\end{itemize}

Observations:
\begin{itemize}
\item $Y_t \vert X_{t} \sim h(y_t | x_t ; \zeta)$
\end{itemize}

Here, $g_1(\cdot)$ and $g(\cdot)$ are appropriately defined probability density functions depending on parameters $\xi$ and $h(\cdot)$ is an appropriately defined probability density function or probability mass function depending on parameters $\zeta$.

Define $\theta = (\xi, \zeta)$ to be the full set of model parameters.




## Locally Level Gaussian Noise Model
In order to demonstrate that the approximation is reasonable we evaluate the predictive accuracy under an analytically tractable model, the locally level Gaussian model. This model takes the form 
$$X_t \sim N(X_{t-1},\sigma_1^2)$$
$$Y_t \sim N(X_t, \sigma_2^2)$$


```{r,echo=FALSE}
library(dlm)

data <- seq(1,10) + rnorm(10,0,1)

mod <- dlmModPoly(1,dV = .1,dW=10)
filt<-dlmFilter(data,mod)

v <- dropFirst(unlist(dlmSvd2var(filt$U.C, filt$D.C)))

n_part <- 10

#Sys.setenv(PATH = paste("/Users/gcgibson/anaconda/bin", Sys.getenv("PATH"), sep=":"))
exec_str <- 'python /home/gcgibson/ssvgd/python/locally_level_gaussian.py '
#exec_str <- 'python python/locally_level_gaussian.py '
exec_str <- paste(exec_str, toString(data))
print (exec_str)
ssvgdForecasts <- system(exec_str,intern=TRUE,wait = TRUE)

#ssvgdForecasts <- strsplit(ssvgdForecasts,",")
#ssvgdForecasts <- as.numeric(unlist(ssvgdForecasts))
#ssvgdForecasts
count <- 1
ssvgdForecasts <- strsplit(ssvgdForecasts[1],",")
ssvgdForecasts <- unlist(ssvgdForecasts)
first_state_forecasts <- c()

for (i in seq(1,length(ssvgdForecasts)) ){
  first_state_forecasts <- c(first_state_forecasts,as.numeric(ssvgdForecasts[i]))
}


aggregate_forecast <- matrix(first_state_forecasts,nrow=length(data),ncol=n_part,byrow = TRUE)


meanSsvgdForecasts <-c()
lowPiSsvgdForecasts <- c()
highPiSsvgdForecasts <- c()

for (i in 1:nrow(aggregate_forecast)){
  meanSsvgdForecasts <- c(meanSsvgdForecasts,mean(aggregate_forecast[i,]))
  srted <- sort(aggregate_forecast[i,])
  lowPiSsvgdForecasts <- c(lowPiSsvgdForecasts,srted[round(.05*length(srted))+1])
  highPiSsvgdForecasts <- c(highPiSsvgdForecasts,srted[round(.95*length(srted))-1])
}

filter_results <- c(dropFirst(filt$f),data[length(data)])

library(ggplot2)

p1<- ggplot() 
p1<- p1+ geom_line(data = data.frame(x=seq(1,length(filter_results)),y=filter_results), aes(x = x, y = y), color = "red") +
  
  geom_line(data=data.frame(x=seq(1,length(data)),y=data), aes(x = x, y = y), color = "cornflowerblue") +
  geom_ribbon(data=data.frame(x=seq(1,length(data)),y=filter_results),aes(x=x,ymin=filter_results-1.96*sqrt(v),ymax=filter_results+1.96*sqrt(v)),alpha=0.3)+
  xlab('data_date') +
  ylab('count') + ylim(low=-5,high=15) 
#print(p1)


p<- ggplot() 
p<- p+ geom_line(data = data.frame(x=seq(1,length(meanSsvgdForecasts)),y=meanSsvgdForecasts), aes(x = x, y = y), color = "red") +
  
  geom_line(data=data.frame(x=seq(1,length(data)),y=data), aes(x = x, y = y), color = "cornflowerblue") +
  geom_ribbon(data=data.frame(x=seq(1,length(data)),y=meanSsvgdForecasts),aes(x=x,ymin=lowPiSsvgdForecasts,ymax=highPiSsvgdForecasts,alpha=0.3))+
  xlab('data_date') +
  ylab('count')+ ylim(low=-5,high=15) 
#print(p)

library(cowplot)
plot_grid(p, p1, labels = c("SVGD","KF"))
ggsave(filename="ssvgd_locally_level.pdf", plot=p)
ggsave(filename="pf_locally_level.pdf", plot=p1)
```

## Poisson Observation Model With Seasonal State-Space Dynamics

In order to evaluate the performance on more involved dynamics we consider the following state-space model.
$$\begin{pmatrix} X_{t,1} \\ X_{t,2} \end{pmatrix} = \begin{pmatrix} cos(2\pi/s) & sin(2\pi/s) \\ -sin(2\pi/s) & cos(2\pi/s) \end{pmatrix} \begin{pmatrix} X_{t-1,1} \\ X_{t-1,2} \end{pmatrix} $$
$$Y_t \sim Pois(e^{X_{t,1}})$$

```{r,echo = FALSE}
require(rbiips)
library(MCMCpack)
library(ggplot2)

ma_data <- round((sin(seq(10))**2)*10+10)

seasonal_model_file = '/home/gcgibson/ssvgd/bug_files/seasonal_pois.bug' # BUGS model filename

seasonal_t_max = length(ma_data)
n_burn = 5000 # nb of burn-in/adaptation iterations
n_iter = 10000 # nb of iterations after burn-in
thin = 5 # thinning of MCMC outputs
n_part = 50 # nb of particles for the SMC
latent_names = c('x') # names of the variables updated with SMC and that need to be monitored

inits = list(-2)

seasonality =6
G = matrix(c(cos(2*pi/seasonality),sin(2*pi/seasonality),-sin(2*pi/seasonality),cos(2*pi/seasonality)), nrow=2, byrow=TRUE)

#setting the mean value of the initial count to 1400
seasonal_data = list(t_max=seasonal_t_max, y = ma_data,  G = G, mean_sigma_init = c(0,0), cov_sigma_init=.001*diag(2) ,mean_x_init=c(log(1400),log(1400)))
seasonal_model = biips_model(seasonal_model_file, data=seasonal_data,sample_data = FALSE)

##fixing variance for now, will extend model to handle inference over variance later

n_part = 100000 # Number of particles
variables = c('x1','x2','x3','x4') # Variables to be monitored
seasonal_out_smc = biips_smc_samples(seasonal_model, variables, n_part)
seasonal_model_summary = biips_summary(seasonal_out_smc, probs=c(.025, .975))

#Sys.setenv(PATH = paste("/Users/gcgibson/anaconda/bin", Sys.getenv("PATH"), sep=":"))

exec_str <- 'python /home/gcgibson/ssvgd/python/seasonal.py '
exec_str <- paste(exec_str, toString(ma_data))
print (exec_str)
ssvgdForecasts <- system(exec_str,intern=TRUE,wait = TRUE)


#ssvgdForecasts <- strsplit(ssvgdForecasts,",")
#ssvgdForecasts <- as.numeric(unlist(ssvgdForecasts))
#ssvgdForecasts
count <- 1
ssvgdForecasts <- strsplit(ssvgdForecasts[1],",")
ssvgdForecasts <- unlist(ssvgdForecasts)
first_state_forecasts <- c()

for (i in seq(1,length(ssvgdForecasts),2) ){
  first_state_forecasts <- c(first_state_forecasts,as.numeric(ssvgdForecasts[i]))
}

aggregate_forecast <- matrix(first_state_forecasts,nrow=10,ncol=10,byrow = TRUE)

meanSsvgdForecasts <- c()
lowPiSsvgdForecasts <- c()
highPiSsvgdForecasts <- c()

for (i in 1:nrow(aggregate_forecast)){
  meanSsvgdForecasts <- c(meanSsvgdForecasts,mean(aggregate_forecast[i,]))
  srted <- sort(aggregate_forecast[i,])
  lowPiSsvgdForecasts <- c(lowPiSsvgdForecasts,srted[round(.05*length(srted))+1])
  highPiSsvgdForecasts <- c(highPiSsvgdForecasts,srted[round(.95*length(srted))-1])
}

lowPf <- pmax(0,seasonal_model_summary$x1$f$quant$`0.025`)
highPf <- pmin(log(100),seasonal_model_summary$x1$f$quant$`0.975`)


p1<- ggplot() 
p1<- p1+ geom_line(data = data.frame(x=seq(1,length(seasonal_model_summary$x1$f$mean)),y=exp(seasonal_model_summary$x1$f$mean)), aes(x = x, y = y), color = "red") +
  
  geom_line(data=data.frame(x=seq(1,length(ma_data)),y=ma_data), aes(x = x, y = y), color = "cornflowerblue") +
  geom_ribbon(data=data.frame(x=seq(1,length(ma_data)),y=exp(seasonal_model_summary$x1$f$mean)),aes(x=x,ymin=exp(lowPf),ymax=exp(highPf)),alpha=0.3)+
  xlab('data_date') +
  ylab('count') + ylim(low=-10,high=110) 
#print(p1)




p<- ggplot() 
p<- p+ geom_line(data = data.frame(x=seq(1,length(meanSsvgdForecasts)),y=exp(meanSsvgdForecasts)), aes(x = x, y = y), color = "red") +
  
  geom_line(data=data.frame(x=seq(1,length(ma_data)),y=ma_data), aes(x = x, y = y), color = "cornflowerblue") +
  geom_ribbon(data=data.frame(x=seq(1,length(ma_data)),y=exp(meanSsvgdForecasts)),aes(x=x,ymin=exp(lowPiSsvgdForecasts),ymax=exp(highPiSsvgdForecasts),alpha=0.3))+
  xlab('data_date') +
  ylab('count') +ylim(low=-10,high=110) 
#print(p)

library(cowplot)
plot_grid(p, p1, labels = c("SVGD","PF"))
ggsave(filename="ssvgd_seasonal.pdf", plot=p)
ggsave(filename="pf_seasonal.pdf", plot=p1)

```

## Divergent Particle Filter

We next investigate the ability of SSVGD to perform in the presence of poor initialization. This is a well known issue with current particle filter implementations: starting far from a plausible value of $x_0$ forces all particles to receive weight $0$ under the likelihood, leading to a degenerate filtering distribution. However, under SSVGD, we can simply increase the number of iterations, allowing for arbitrarily poor starting points. Standard particle filtering algorithms use effective sample size as a measure of degeneracy. This is commonly defined as $$S^{pf}_{eff} = \frac{1}{\sum_i (w_t^i)^2}$$. The common rule of thumb is to not allow this quantity to drop below 50. The natural translation of this metric into particle filtering is compute the same metric based on the samples obtained by SSVGD. 


```{r, echo=FALSE}
library(ggplot2)
require(rbiips)
library(MCMCpack)
locally_level_model_file = '/home/gcgibson/ssvgd/bug_files/locally_level_1.bug' # BUGS model filename

ma_data = c(1,4,0,3,10,20,4,3,7,40,1)
t_max = length(ma_data)


#setting the mean value of the initial count to 1400
locally_level_data = list(t_max=t_max, y = ma_data,  mean_x_init=-2)
locally_level_model = biips_model(locally_level_model_file, data=locally_level_data,sample_data = FALSE)
n_part = 1000 # Number of particles
variables = c('x') # Variables to be monitored
mn_type = 'fs'; rs_type = 'stratified'; rs_thres = 0.5 # Optional parameters


out_smc = biips_smc_samples(locally_level_model, variables, n_part,
                            type=mn_type, rs_type=rs_type, rs_thres=rs_thres)
diag_smc = biips_diagnosis(out_smc)

summ_smc = biips_summary(out_smc, probs=c(.025, .975))
print (summ_smc$x$f$mean)

#Sys.setenv(PATH = paste("/Users/gcgibson/anaconda/bin", Sys.getenv("PATH"), sep=":"))

exec_str <- 'python /home/gcgibson/ssvgd/python/ssvgd.py '
exec_str <- paste(exec_str, toString(ma_data))
print (exec_str)
ssvgdForecasts <- system(exec_str,intern=TRUE,wait = TRUE)


npart <- 1000

count <- 1
ssvgdForecasts <- strsplit(ssvgdForecasts[1],",")
ssvgdForecasts <- unlist(ssvgdForecasts)
first_state_forecasts <- c()
weights <- c()

for (i in seq(1,round(length(ssvgdForecasts)/2)) ){
  first_state_forecasts <- c(first_state_forecasts,as.numeric(ssvgdForecasts[i]))
}

for (i in seq(round(length(ssvgdForecasts)/2)+1,length(ssvgdForecasts)) ){
  weights <- c(weights,as.numeric(ssvgdForecasts[i]))
}


aggregate_forecast <- matrix(first_state_forecasts,nrow=length(ma_data),ncol=npart,byrow = TRUE)
weights <- matrix(weights,nrow=length(ma_data),ncol=npart,byrow=TRUE)

meanSsvgdForecasts <-c()
lowPiSsvgdForecasts <- c()
highPiSsvgdForecasts <- c()
varSsvgdForecasts <- c()

for (i in 1:nrow(aggregate_forecast)){
  meanSsvgdForecasts <- c(meanSsvgdForecasts,mean(aggregate_forecast[i,]))
  srted <- sort(aggregate_forecast[i,])
  lowPiSsvgdForecasts <- c(lowPiSsvgdForecasts,srted[round(.05*length(srted))+1])
  highPiSsvgdForecasts <- c(highPiSsvgdForecasts,srted[round(.95*length(srted))-1])
}

lowPf <-summ_smc$x$f$quant$`0.025`
highPf <- summ_smc$x$f$quant$`0.975`



p1<- ggplot() 
p1<- p1+ geom_line(data = data.frame(x=seq(1,length(summ_smc$x$f$mean)),y=summ_smc$x$f$mean), aes(x = x, y = y), color = "red") +
  
  geom_line(data=data.frame(x=seq(1,length(ma_data)),y=ma_data), aes(x = x, y = y), color = "cornflowerblue") +
  geom_ribbon(data=data.frame(x=seq(1,length(ma_data)),y=summ_smc$x$f$mean),aes(x=x,ymin=lowPf,ymax=highPf),alpha=0.3)+
  xlab('data_date') +
  ylab('count')# + ylim(low=-10,high=10) 
#print(p1)




p<- ggplot() 
p<- p+ geom_line(data = data.frame(x=seq(1,length(meanSsvgdForecasts)),y=meanSsvgdForecasts), aes(x = x, y = y), color = "red") +
  
  geom_line(data=data.frame(x=seq(1,length(ma_data)),y=ma_data), aes(x = x, y = y), color = "cornflowerblue") +
  geom_ribbon(data=data.frame(x=seq(1,length(ma_data)),y=meanSsvgdForecasts),aes(x=x,ymin=lowPiSsvgdForecasts,ymax=highPiSsvgdForecasts,alpha=0.3))+
  xlab('data_date') +
  ylab('count') #+ylim(low=-10,high=10) 
#print(p)

library(cowplot)
plot_grid(p, p1, labels = c("SVGD","PF"))
ggsave(filename="ssvgd_divergent.pdf", plot=p)
ggsave(filename="pf_divergent.pdf", plot=p1)

print (varSsvgdForecasts)
weights <- matrix(weights,nrow=length(ma_data),ncol=npart,byrow=TRUE)

effect_size <- c()
for (w in 1:nrow(weights)){
  tmp <- weights[w,]
  tmp <- tmp/sum(tmp)
  
  effect_size <- c(effect_size, 1/sum(tmp**2))
}

```


### Diagnostics

Standard particle filtering algorithms use effective sample size as a measure of degeneracy. This is commonly defined as $$S_{eff} = \frac{1}{\sum_i (w_i)^2}$$. The common rule of thumb is to not allow this quantity to fall below 50. Indeed, software implementations such as Biips throws an error if the number of effective particles falls below 50. We compute the effective sample size in an analogous way to the particle filter, where $w_i$ is defined as in the SIR particle filter. 


```{r}
library(knitr)

ess_df <- data.frame(seq(1:length(ma_data)),effect_size,out_smc$x$f$ess)
colnames(ess_df) <- c("t","SSVGD ESS", "PF ESS")
kable(ess_df,caption="Effective Sample size")
write.csv(ess_df, file = "ess_df.csv")
```

#### Results
In order to assess the accuracy of SSVGD we consider multiple different evaluation metrics. We evaluate the forecasts on both mean-square-error and log-score. Log-scores were computed as follows,

$$p(y_{1:n}) = \int_{x_{1:n}} p(y_{1:n} | x_{1:n}) p(x_{1:n}) d_{x_{1:n}}$$

This can be approximated as 

$$\frac{1}{k}\sum_{i=1}^k p(y_{1:n} | x^{(i)}_{1:n})$$
That is, we take a full trajectory $x_{1:n}$ and compute the log-probability of $y_{1:n}$. Note that this is equivalent to taking the average weight at each step. 

```{r}
log_p <- c()
for (i in 1:nrow(weights)){
  log_p <- c(log_p,mean(weights[i,]))
}

exec_str <- 'python /home/gcgibson/ssvgd/python/kf_true.py '
true_ll <- system(exec_str,intern=TRUE,wait = TRUE)



ll_df <- data.frame(sum(log_p),out_smc$log_marg_like, as.double(true_ll[length(true_ll)]))
colnames(ll_df) <- c("SSVGD","PF","KF")
kable(ll_df,caption="Log Score")
write.csv(ll_df, file = "ll_df.csv")

```


## Discussion
Sequential Stein Variational Gradient Descent offers a useful alternative to traditional particle filtering. We are able to increase the effective sample size using the same number of particles. We are also able to approximate the true likelihood of the Kalman Filter with a small number of particles. Further work is required to understand the impact of the bandwith heuristic on the resulting predictive intervals. 


## Bibliography 


