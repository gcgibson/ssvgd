---
title: 'Sequential Stein Variational Gradient Descent for Time Series Model Estimation'
author: "Gibson, Reich, and Ray in some order"
date: "December 3, 2017"
output:
  pdf_document:
    fig_height: 2.7
    fig_width: 6.5
    keep_tex: yes
header-includes:
   - \usepackage{multicol}
   - \usepackage{amssymb}
---


```{r, include=FALSE}
require(knitr)
library(tidyverse)
library(RefManageR)

options(digits=4)

opts_chunk$set(
  tidy=FALSE,     # display code as typed
  size="small"    # slightly smaller font for code
)
```

# Introduction

Particle filtering suffers from two main practical disadvantages. The first is particle depletion, where the number of effective particles with non-neglibile weight becomes too small. This has the effect of concentrating the mass around a small number of particles, leading to poor estimates of the target distribution. The second is the running time of the algorithm. A cursory analysis reveals that each particle is updated once per time step in the time series, and once per re-sampling step, to mitigate the issue above. If we imagine the order of particles is close to the length of the time series, we see that run-time is $O(n^3)$.

We propose another approach that we hope will do better than particle filtering in practice.  In this approach, Stein Variational Gradient Descent (SVGD) is used to sequentially estimate the distribution of state variables in each time step, conditional on observed data up through that time.  This method should overcome problems with particle depletion and excessive run-times for long time-series.

## Overview of SVGD
Stein Variational Gradient Descent can be used to estimate a continuous distribution by a set of samples. By iteratively transporting samples from an initial distribution in the direction of the likelihood, we are able to generate a sample from the target distribution. The usefullness of this approximation is apparent in Bayesian statistics, where the usually intractable normalizing constant disappears in the gradient. The particles are subject to the following gradient ascent procedure. 

$$x_t^{(i)} \leftarrow x_{t-1}^{(i)} + \frac{1}{n}\sum_{j=1}^n[ k(x_j,x_{t-1}^{(i)})*\nabla log\ p(x_j) + \nabla k(x_j,,x_{t-1}^{(i)})]$$



## Sequential Stein Variational Gradient Descent 
Suppose we are given a time series $Y_1,Y_2,...,Y_t$ for $Y \in \mathbb{R}$. We model the sequence as a state-space model parameterized by an observation density $p(y_t | x_t)$ and a transition density $p(x_t | x_{t-1})$ Figure 1.

![Caption for the picture.](/Users/gcgibson/Desktop/ssm.png)


We are interested in the filtering distribution $p(x_1,...,x_n | y_1,...,y_n)$ which by Bayes formula is $$p(x_1,...,x_n | y_1,...,y_n) = \frac{p(y_1,...,y_n | x_1,...,x_n) p(x_1,...,x_n)}{Z}$$.

Because computing the normalizing constant $Z$ is intractable for many choices of $p(y_t | x_t)$ and $p(x_t | x_{t-1})$, we must resort to monte carlo algorithms. The classic approach that incorporates the sequential nature of the data is given by the particle filtering algorithm. Particle filtering approximates the filtering density using sequential importance sampling. We instead focus on the following recursion. 

$$p(x_t | y_{1:t}) = \int p(x_{0:t} | y_{1:t})d_{x_0:t-1}$$
$$=\frac{p(y_t | x_t)}{\int p(y_t|x_t)p(x_t | y_{1:t-1})dx_t}p(x_t | y_{1:t-1})$$

$$\propto p(y_t|x_t)p(x_t | y_{1:t-1})$$
$$\propto p(y_t|x_t)p(x_t | y_{1:t-1})$$
$$\propto p(y_t|x_t)\int_{x_{t-1}}p(x_t,x_{t-1} | y_{1:t-1})d_{x_{t-1}}$$

$$\propto p(y_t|x_t)\int_{x_{t-1}}p(x_t |x_{t-1} )p(x_{t-1}| y_{1:t-1})d_{x_{t-1}}$$

which we can approximate using svgd as 
$$\approx p(y_t|x_t) \frac{1}{n}\sum_{i=1}^n p(x_t | x_{t-1}^{(i)})$$
We can now estimate $p(x_{t+1}|y_{1:t+1})$ using the same algebra as above. 
(proof in apendix A) 






## Model Structure

States:
\begin{itemize}
\item $X_1 \sim g_1(x_1 ; \xi)$
\item $X_t \vert X_{t-1} \sim g(x_t \vert x_{t - 1} ; \xi)$ for all $t = 2, \ldots, T$
\end{itemize}

Observations:
\begin{itemize}
\item $Y_t \vert X_{t} \sim h(y_t | x_t ; \zeta)$
\end{itemize}

Here, $g_1(\cdot)$ and $g(\cdot)$ are appropriately defined probability density functions depending on parameters $\xi$ and $h(\cdot)$ is an appropriately defined probability density function or probability mass function depending on parameters $\zeta$.

Define $\theta = (\xi, \zeta)$ to be the full set of model parameters.



## Filtering

There are two types of filtering:

1. sample of particles $x_{1:T}^{(k)} \sim f(x_{1:T} | y_{1:T})$
2. sample of particles $x_{t}^{(k)} \sim f(x_{t} | y_{1:t})$ for each $t = 1, \ldots, T$

Let's look at the second one.  Assume we have a sample $x_{t-1}^{(k)} \sim f(x_{t-1} | y_{1:t-1})$

\begin{align*}
p(x_{t} | y_{1:t}) &= \frac{f(x_t, y_t | y_{1:t-1})}{f(y_t | y_{1:t-1})} \\
 &\propto f(x_t, y_t | y_{1:t-1}) \\
 &= f(y_t | x_t) f(x_t | y_{1:t-1}) \\
 &= f(y_t | x_t) \int f(x_t, x_{t-1} | y_{1:t-1}) d x_{t-1} \\
 &= f(y_t | x_t) \int f(x_t | x_{t - 1}) f(x_{t-1} | y_{1:{t-1}}) dx_{t-1} \\
 &\approx f(y_t | x_t) \sum_{x_{t-1}^{(k)}} f(x_t | x_{t - 1}^{(k)})
\end{align*}

So $\log\{p(x_{t} | y_{1:t})\}$ is approximately proportional to $\log\{f(y_t | x_t)\} + \log\{\sum_{x_{t-1}^{(k)}} f(x_t | x_{t - 1}^{(k)})\}$


## Locally Level Gaussian Noise Model
In order to demonstrate that the approximation is reasonable we evaluate the predictive accuracy under an analytically tractable model, the locally level Gaussian model. This model takes the form 
$$X_t \sim N(X_{t-1},\sigma_1^2)$$
$$Y_t \sim N(X_t, \sigma_2^2)$$


```{r}
library(dlm)

data <- seq(1,10) + rnorm(10,0,1)

mod <- dlmModPoly(1,dV = .1,dW=10)
filt<-dlmFilter(data,mod)

v <- dropFirst(unlist(dlmSvd2var(filt$U.C, filt$D.C)))

n_part <- 10

Sys.setenv(PATH = paste("/Users/gcgibson/anaconda/bin", Sys.getenv("PATH"), sep=":"))
exec_str <- 'python /Users/gcgibson/Stein-Variational-Gradient-Descent/python/locally_level_gaussian.py '
#exec_str <- 'python python/locally_level_gaussian.py '
exec_str <- paste(exec_str, toString(data))
print (exec_str)
ssvgdForecasts <- system(exec_str,intern=TRUE,wait = TRUE)

#ssvgdForecasts <- strsplit(ssvgdForecasts,",")
#ssvgdForecasts <- as.numeric(unlist(ssvgdForecasts))
#ssvgdForecasts
count <- 1
ssvgdForecasts <- strsplit(ssvgdForecasts[1],",")
ssvgdForecasts <- unlist(ssvgdForecasts)
first_state_forecasts <- c()

for (i in seq(1,length(ssvgdForecasts)) ){
  first_state_forecasts <- c(first_state_forecasts,as.numeric(ssvgdForecasts[i]))
}


aggregate_forecast <- matrix(first_state_forecasts,nrow=length(data),ncol=n_part,byrow = TRUE)


meanSsvgdForecasts <-c()
lowPiSsvgdForecasts <- c()
highPiSsvgdForecasts <- c()

for (i in 1:nrow(aggregate_forecast)){
  meanSsvgdForecasts <- c(meanSsvgdForecasts,mean(aggregate_forecast[i,]))
  srted <- sort(aggregate_forecast[i,])
  lowPiSsvgdForecasts <- c(lowPiSsvgdForecasts,srted[round(.05*length(data))+1])
  highPiSsvgdForecasts <- c(highPiSsvgdForecasts,srted[round(.95*length(data))-1])
}

filter_results <- c(dropFirst(filt$f),11)

library(ggplot2)

p1<- ggplot() 
p1<- p1+ geom_line(data = data.frame(x=seq(1,length(filter_results)),y=filter_results), aes(x = x, y = y), color = "red") +
  
  geom_line(data=data.frame(x=seq(1,length(data)),y=data), aes(x = x, y = y), color = "cornflowerblue") +
  geom_ribbon(data=data.frame(x=seq(1,length(data)),y=filter_results),aes(x=x,ymin=filter_results-1.96*sqrt(v),ymax=filter_results+1.96*sqrt(v)),alpha=0.3)+
  xlab('data_date') +
  ylab('count') #+ ylim(low=-2,high=10) 
print(p1)


p<- ggplot() 
p<- p+ geom_line(data = data.frame(x=seq(1,length(meanSsvgdForecasts)),y=meanSsvgdForecasts), aes(x = x, y = y), color = "red") +
  
  geom_line(data=data.frame(x=seq(1,length(data)),y=data), aes(x = x, y = y), color = "cornflowerblue") +
  geom_ribbon(data=data.frame(x=seq(1,length(data)),y=meanSsvgdForecasts),aes(x=x,ymin=lowPiSsvgdForecasts,ymax=highPiSsvgdForecasts,alpha=0.3))+
  xlab('data_date') +
  ylab('count')# +ylim(low=-10,high=100) 
print(p)

library(cowplot)
plot_grid(p, p1, labels = c("SVGD","PF"))
```


## Simulation Studies

We will do several simulation studies, divided into 2 groups:

1. illustrating scenarios in which common particle filtering methods struggle, but SSVGD has better chances
    a. bad initalization
    a. normal-poisson filtering, seasonal model
    a. one other, more complex?
1. demonstrating accuracy (compare to true states, exactly computed filtered states, and likelihood)
    a. Kalman filter
    a. something nonlinear where we can do exact computations (brute force for short time series??)?

### 1. a. Bad Initialization

#### Simulation Study Design

Paragraph with data generating process, written with math.

Paragraph describing settings for simulation, e.g. number of simulation runs, length of time series generated, etc.

Paragraph describing different methods in comparison.  2 PF implementations, one non-linear KF implementation, and SSVGD

#### Results

Paragraph, referencing one figure and one table, summarizing results

### 1. b. Normal-Poisson seasonal model

#### Simulation Study Design

Paragraph with data generating process, written with math.

Paragraph describing settings for simulation, e.g. number of simulation runs, length of time series generated, etc.

Paragraph describing different methods in comparison.  2 PF implementations, one non-linear KF implementation, and SSVGD

#### Results

Paragraph, referencing one figure and one table, summarizing results

### 1. c. One other, more complex?

#### Simulation Study Design

Paragraph with data generating process, written with math.

Paragraph describing settings for simulation, e.g. number of simulation runs, length of time series generated, etc.

Paragraph describing different methods in comparison.  2 PF implementations, one non-linear KF implementation, and SSVGD

#### Results

Paragraph, referencing one figure and one table, summarizing results

### 2. a. Kalman Filter

#### Simulation Study Design

Paragraph with data generating process, written with math.

Paragraph describing settings for simulation, e.g. number of simulation runs, length of time series generated, etc.

Paragraph describing different methods in comparison.  2 PF implementations, one non-linear KF implementation, and SSVGD

#### Results

Paragraph, referencing one figure and one table, summarizing results

### 2. b. something nonlinear where we can do exact computations (brute force for short time series??)?

#### Simulation Study Design

Paragraph with data generating process, written with math.

Paragraph describing settings for simulation, e.g. number of simulation runs, length of time series generated, etc.

Paragraph describing different methods in comparison.  2 PF implementations, one non-linear KF implementation, and SSVGD

#### Results

Paragraph, referencing one figure and one table, summarizing results

## Application

Example model with real data.  fairly real model, but not thaaaaaat complex.

## Discussion


## Bibliography 

